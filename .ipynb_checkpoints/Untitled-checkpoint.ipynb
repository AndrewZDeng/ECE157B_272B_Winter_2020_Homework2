{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "d:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "d:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "d:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "d:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "d:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "d:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "d:\\program files\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "d:\\program files\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "d:\\program files\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "d:\\program files\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "d:\\program files\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "d:\\program files\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.load(\"train_data.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_x.reshape(train_x.shape[0], 64, 64, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.load(\"train_label.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_y(train_y):\n",
    "    type_list = []\n",
    "    label_y = np.zeros((len(train_y), 6), dtype=int)\n",
    "    labl = 0\n",
    "    idx = 0\n",
    "    for item in train_y:\n",
    "        if np.size(type_list) == 0:\n",
    "            type_list.append([item, labl])\n",
    "            label_y[idx, labl] = 1\n",
    "            labl += 1\n",
    "        else:\n",
    "            record = 0\n",
    "            pos = 0\n",
    "            for inst in type_list:\n",
    "                if item == inst[0]:\n",
    "                    label_y[idx, pos] = 1\n",
    "                    record = 1\n",
    "                    break\n",
    "                pos += 1\n",
    "            if record == 0:\n",
    "                type_list.append([item, labl])\n",
    "                label_y[idx, labl] = 1\n",
    "                labl += 1\n",
    "        idx += 1\n",
    "    return label_y, type_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_y, type_list = transform_y(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=16, kernel_size=(3,3), strides=(1,1), padding='same', input_shape=(64,64,1), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3,3)))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "#model.add(Conv2D(filters=32, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\n",
    "#model.add(MaxPooling2D(pool_size=(3,3)))\n",
    "#model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3,3)))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "#model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adagrad', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_training, X_test, y_training, y_test = train_test_split(X_train, label_y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 300 samples, validate on 100 samples\n",
      "Epoch 1/40\n",
      "300/300 [==============================] - 0s 1ms/step - loss: 1.3884 - accuracy: 0.6933 - val_loss: 1.0617 - val_accuracy: 0.7600\n",
      "Epoch 2/40\n",
      "300/300 [==============================] - 0s 347us/step - loss: 0.8344 - accuracy: 0.7233 - val_loss: 0.5530 - val_accuracy: 0.8700\n",
      "Epoch 3/40\n",
      "300/300 [==============================] - 0s 326us/step - loss: 0.5324 - accuracy: 0.7733 - val_loss: 0.3835 - val_accuracy: 0.8800\n",
      "Epoch 4/40\n",
      "300/300 [==============================] - 0s 340us/step - loss: 0.4540 - accuracy: 0.8367 - val_loss: 0.3699 - val_accuracy: 0.8500\n",
      "Epoch 5/40\n",
      "300/300 [==============================] - 0s 322us/step - loss: 0.3978 - accuracy: 0.8367 - val_loss: 0.3299 - val_accuracy: 0.8600\n",
      "Epoch 6/40\n",
      "300/300 [==============================] - 0s 327us/step - loss: 0.3468 - accuracy: 0.8900 - val_loss: 0.2954 - val_accuracy: 0.9100\n",
      "Epoch 7/40\n",
      "300/300 [==============================] - 0s 334us/step - loss: 0.3151 - accuracy: 0.8767 - val_loss: 0.2578 - val_accuracy: 0.9100\n",
      "Epoch 8/40\n",
      "300/300 [==============================] - 0s 337us/step - loss: 0.2860 - accuracy: 0.9000 - val_loss: 0.3041 - val_accuracy: 0.8400\n",
      "Epoch 9/40\n",
      "300/300 [==============================] - 0s 331us/step - loss: 0.2516 - accuracy: 0.9233 - val_loss: 0.2267 - val_accuracy: 0.9200\n",
      "Epoch 10/40\n",
      "300/300 [==============================] - 0s 328us/step - loss: 0.2373 - accuracy: 0.9233 - val_loss: 0.2807 - val_accuracy: 0.9000\n",
      "Epoch 11/40\n",
      "300/300 [==============================] - 0s 338us/step - loss: 0.2150 - accuracy: 0.9400 - val_loss: 0.1964 - val_accuracy: 0.9200\n",
      "Epoch 12/40\n",
      "300/300 [==============================] - 0s 326us/step - loss: 0.1826 - accuracy: 0.9500 - val_loss: 0.1737 - val_accuracy: 0.9500\n",
      "Epoch 13/40\n",
      "300/300 [==============================] - 0s 320us/step - loss: 0.1599 - accuracy: 0.9600 - val_loss: 0.1944 - val_accuracy: 0.9300\n",
      "Epoch 14/40\n",
      "300/300 [==============================] - 0s 324us/step - loss: 0.1591 - accuracy: 0.9667 - val_loss: 0.1808 - val_accuracy: 0.9300\n",
      "Epoch 15/40\n",
      "300/300 [==============================] - 0s 324us/step - loss: 0.1315 - accuracy: 0.9700 - val_loss: 0.1785 - val_accuracy: 0.9400\n",
      "Epoch 16/40\n",
      "300/300 [==============================] - 0s 323us/step - loss: 0.1314 - accuracy: 0.9667 - val_loss: 0.1689 - val_accuracy: 0.9100\n",
      "Epoch 17/40\n",
      "300/300 [==============================] - 0s 340us/step - loss: 0.1133 - accuracy: 0.9733 - val_loss: 0.1912 - val_accuracy: 0.9200\n",
      "Epoch 18/40\n",
      "300/300 [==============================] - 0s 319us/step - loss: 0.1232 - accuracy: 0.9667 - val_loss: 0.1722 - val_accuracy: 0.9100\n",
      "Epoch 19/40\n",
      "300/300 [==============================] - 0s 319us/step - loss: 0.0958 - accuracy: 0.9800 - val_loss: 0.1431 - val_accuracy: 0.9500\n",
      "Epoch 20/40\n",
      "300/300 [==============================] - 0s 320us/step - loss: 0.0897 - accuracy: 0.9833 - val_loss: 0.1411 - val_accuracy: 0.9400\n",
      "Epoch 21/40\n",
      "300/300 [==============================] - 0s 320us/step - loss: 0.1031 - accuracy: 0.9700 - val_loss: 0.1414 - val_accuracy: 0.9400\n",
      "Epoch 22/40\n",
      "300/300 [==============================] - 0s 326us/step - loss: 0.0735 - accuracy: 0.9833 - val_loss: 0.1359 - val_accuracy: 0.9400\n",
      "Epoch 23/40\n",
      "300/300 [==============================] - 0s 330us/step - loss: 0.0877 - accuracy: 0.9767 - val_loss: 0.1322 - val_accuracy: 0.9400\n",
      "Epoch 24/40\n",
      "300/300 [==============================] - 0s 347us/step - loss: 0.0692 - accuracy: 0.9800 - val_loss: 0.1386 - val_accuracy: 0.9400\n",
      "Epoch 25/40\n",
      "300/300 [==============================] - 0s 344us/step - loss: 0.0714 - accuracy: 0.9867 - val_loss: 0.1199 - val_accuracy: 0.9600\n",
      "Epoch 26/40\n",
      "300/300 [==============================] - 0s 323us/step - loss: 0.0663 - accuracy: 0.9900 - val_loss: 0.1272 - val_accuracy: 0.9500\n",
      "Epoch 27/40\n",
      "300/300 [==============================] - 0s 316us/step - loss: 0.0678 - accuracy: 0.9933 - val_loss: 0.1270 - val_accuracy: 0.9600\n",
      "Epoch 28/40\n",
      "300/300 [==============================] - 0s 316us/step - loss: 0.0547 - accuracy: 0.9900 - val_loss: 0.1155 - val_accuracy: 0.9500\n",
      "Epoch 29/40\n",
      "300/300 [==============================] - 0s 323us/step - loss: 0.0562 - accuracy: 0.9933 - val_loss: 0.1191 - val_accuracy: 0.9500\n",
      "Epoch 30/40\n",
      "300/300 [==============================] - 0s 332us/step - loss: 0.0501 - accuracy: 0.9933 - val_loss: 0.1159 - val_accuracy: 0.9500\n",
      "Epoch 31/40\n",
      "300/300 [==============================] - 0s 325us/step - loss: 0.0466 - accuracy: 0.9933 - val_loss: 0.1093 - val_accuracy: 0.9500\n",
      "Epoch 32/40\n",
      "300/300 [==============================] - 0s 356us/step - loss: 0.0472 - accuracy: 0.9933 - val_loss: 0.1178 - val_accuracy: 0.9600\n",
      "Epoch 33/40\n",
      "300/300 [==============================] - 0s 339us/step - loss: 0.0494 - accuracy: 0.9933 - val_loss: 0.1165 - val_accuracy: 0.9600\n",
      "Epoch 34/40\n",
      "300/300 [==============================] - 0s 320us/step - loss: 0.0394 - accuracy: 0.9967 - val_loss: 0.1339 - val_accuracy: 0.9500\n",
      "Epoch 35/40\n",
      "300/300 [==============================] - 0s 303us/step - loss: 0.0429 - accuracy: 0.9967 - val_loss: 0.1176 - val_accuracy: 0.9500\n",
      "Epoch 36/40\n",
      "300/300 [==============================] - 0s 310us/step - loss: 0.0367 - accuracy: 1.0000 - val_loss: 0.1026 - val_accuracy: 0.9600\n",
      "Epoch 37/40\n",
      "300/300 [==============================] - 0s 313us/step - loss: 0.0364 - accuracy: 0.9967 - val_loss: 0.1038 - val_accuracy: 0.9700\n",
      "Epoch 38/40\n",
      "300/300 [==============================] - 0s 313us/step - loss: 0.0387 - accuracy: 0.9900 - val_loss: 0.0979 - val_accuracy: 0.9700\n",
      "Epoch 39/40\n",
      "300/300 [==============================] - 0s 317us/step - loss: 0.0318 - accuracy: 1.0000 - val_loss: 0.0995 - val_accuracy: 0.9700\n",
      "Epoch 40/40\n",
      "300/300 [==============================] - 0s 340us/step - loss: 0.0331 - accuracy: 0.9967 - val_loss: 0.1000 - val_accuracy: 0.9700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1d2866f6ac8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_training, y_training, validation_data=(X_test, y_test), epochs=40, batch_size=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
